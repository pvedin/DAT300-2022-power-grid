{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1c9c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "import pandapower.networks\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8c3b949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if network == 'IEEE-14' then\n",
    "net = pp.networks.case14()\n",
    "#else if network == 'IEEE-30' then\n",
    "#net = pandapower.networks.case30()\n",
    "#else if network == 'IEEE-57' then\n",
    "#net = pandapower.networks.case57()\n",
    "#else if network == 'IEEE-118' then\n",
    "#net = pandapower.networks.case118()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "94579c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if user selects data generation strategy 1\n",
    "pp.rundcpp(net)\n",
    "# state vector x_base\n",
    "x_base = net.res_bus.va_degree.to_numpy()/180*np.pi\n",
    "# number of discrete time steps\n",
    "T = 10000 # user argument\n",
    "# Dx ~ N(0,Cov_Mat)\n",
    "mean = np.zeros(x_base.shape)\n",
    "cov = np.eye(x_base.shape[0])*0.001\n",
    "delta_x_mat = np.transpose(np.random.multivariate_normal(mean, cov**2, size=T))\n",
    "x_base_mat = np.repeat(x_base.reshape((x_base.shape[0],-1)), T, axis=1)\n",
    "x_t_mat = x_base_mat + delta_x_mat\n",
    "x_t_mat[0,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e35c98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_from_node_i_to_node_j\n",
    "# Pij = (1/bij)*(x[i]-x[j])\n",
    "# H[line_id,i] = 1/bij\n",
    "# H[line_id,j] = -1/bij\n",
    "A_real = np.real(net._ppc['internal']['Bbus'].A)\n",
    "#H = np.zeros((net.line.shape[0]+net.trafo.shape[0], net.bus.shape[0]))\n",
    "H = np.zeros((2*(net.line.shape[0]+net.trafo.shape[0]), net.bus.shape[0]))\n",
    "for i in range(0, net.line.shape[0]):\n",
    "    # from flows\n",
    "    H[2*i, net.line.from_bus.values[i]] = -A_real[net.line.from_bus.values[i],net.line.to_bus.values[i]]\n",
    "    H[2*i, net.line.to_bus.values[i]] = A_real[net.line.from_bus.values[i],net.line.to_bus.values[i]]\n",
    "    # to flows\n",
    "    H[2*i+1, net.line.to_bus.values[i]] = -A_real[net.line.to_bus.values[i],net.line.from_bus.values[i]]    \n",
    "    H[2*i+1, net.line.from_bus.values[i]] = A_real[net.line.to_bus.values[i],net.line.from_bus.values[i]]\n",
    "    \n",
    "#for i in range(net.line.shape[0]+1, net.line.shape[0]+net.trafo.shape[0]):\n",
    "for i in range(net.line.shape[0], net.line.shape[0]+net.trafo.shape[0]):\n",
    "    # from flows\n",
    "    H[2*i, net.trafo.hv_bus.values[i-net.line.shape[0]]] = -A_real[net.trafo.hv_bus.values[i-net.line.shape[0]],net.trafo.lv_bus.values[i-net.line.shape[0]]]\n",
    "    H[2*i, net.trafo.lv_bus.values[i-net.line.shape[0]]] = A_real[net.trafo.hv_bus.values[i-net.line.shape[0]],net.trafo.lv_bus.values[i-net.line.shape[0]]]\n",
    "    # to flows\n",
    "    H[2*i+1, net.trafo.lv_bus.values[i-net.line.shape[0]]] = -A_real[net.trafo.lv_bus.values[i-net.line.shape[0]],net.trafo.hv_bus.values[i-net.line.shape[0]]]\n",
    "    H[2*i+1, net.trafo.hv_bus.values[i-net.line.shape[0]]] = A_real[net.trafo.lv_bus.values[i-net.line.shape[0]],net.trafo.hv_bus.values[i-net.line.shape[0]]]\n",
    "    \n",
    "#H = np.vstack((H,np.eye(net.bus.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7a4cb85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 14)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6568990b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "0cd880bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10000)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z_t = H @ x_t + noise\n",
    "mean = np.zeros(H.shape[0])\n",
    "cov = np.eye(H.shape[0])*0.01\n",
    "noise_mat = np.transpose(np.random.multivariate_normal(mean, cov, size=T))\n",
    "z_t_mat = np.matmul(H, x_t_mat) + noise_mat\n",
    "z_t_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "abe9796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[0.       ]\n",
      " [0.0773338]]\n",
      "2\n",
      "[[0.        ]\n",
      " [0.05504013]]\n",
      "3\n",
      "[[0.        ]\n",
      " [0.02459395]]\n",
      "4\n",
      "[[0.00000000e+00]\n",
      " [4.57966998e-16]]\n"
     ]
    }
   ],
   "source": [
    "# target with matching pursuit \n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "I = [i for i in range(0,H.shape[1])]\n",
    "I_fixed = [0,4] # 0 always there, but \"1,2,3\" could have been \"2,4,9\"\n",
    "I_free = [i for i in I if I.index(i) not in I_fixed]\n",
    "\n",
    "H_s = H[:,I_free]\n",
    "H_s_transpose = np.transpose(H_s)\n",
    "B_s = H_s @ np.linalg.inv(H_s_transpose @ H_s) @ H_s_transpose - np.eye(H_s.shape[0])\n",
    "c = np.zeros((H.shape[1],))\n",
    "for i in I_fixed[1:]:\n",
    "    c[i] = 0.1\n",
    "y = B_s @ H[:,I_fixed] @ c[I_fixed]\n",
    "\n",
    "for n in range(1,H.shape[0]+1):\n",
    "    print(n)\n",
    "    # Bs*a=y, solve for a where cardinality(a) = n_nonzero_coefs\n",
    "    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n, normalize=False)\n",
    "    omp.fit(B_s, y)\n",
    "    a = omp.coef_\n",
    "    # a = H*c => c = (H^T * H)^(-1) * H^T * a\n",
    "    resulting_c = np.linalg.inv(np.transpose(H[:,1:]) @ H[:,1:]) @ np.transpose(H[:,1:]) @ a\n",
    "    resulting_c = np.hstack((0,resulting_c))\n",
    "    print((c[I_fixed]-resulting_c[I_fixed]).reshape(-1,1))\n",
    "    if np.abs(np.max(c[I_fixed]-resulting_c[I_fixed])) < 1e-4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c659d740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "673d7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning for adding constraints: zero or small (< 1e-13) coefficients, ignored\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (win64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 110 rows, 74 columns and 422 nonzeros\n",
      "Model fingerprint: 0x779edeee\n",
      "Coefficient statistics:\n",
      "  Matrix range     [9e-04, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e-01, 1e+02]\n",
      "Presolve removed 104 rows and 59 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 6 rows, 15 columns, 90 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    2.2304000e-02   3.227991e-03   0.000000e+00      0s\n",
      "       1    2.8221000e-02   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 1 iterations and 0.02 seconds (0.00 work units)\n",
      "Optimal objective  2.822100000e-02\n"
     ]
    }
   ],
   "source": [
    "# Targeted least effort based on norm-1\n",
    "# min norm-1(a) with respect to a and c, where a is in z+a and c is in x+c\n",
    "# subject to:\n",
    "# a = H*c -> B_s*a = y\n",
    "# a[k] = 0.1 where this is the initial desire of the adversary i.e. add 0.1 to the k-th measurement\n",
    "# all entries of a and c should be within some limits\n",
    "# -0.1 <= a[i] <= 0.1 for all i going from 0 to #meas-1\n",
    "# -0.1 <= c[i] <= 0.1 for all i going from 0 to #nodes-1\n",
    "\n",
    "# input params: H, z, k as integer between 0 and H.shape[0]-1, delta_a, a_lower_bound, a_upper_bound, \n",
    "# c_lower_bound, c_upper_bound\n",
    "\n",
    "from gurobipy import *\n",
    "\n",
    "delta_a = 0.1\n",
    "k = 4\n",
    "a_lower_bound, a_upper_bound, c_lower_bound, c_upper_bound = -100, 100, -100, 100\n",
    "\n",
    "# new material for targerted with norm 1\n",
    "I = [i for i in range(0,H.shape[1])]\n",
    "I_fixed = [0,k] # 0 always there, but \"1,2,3\" could have been \"2,4,9\"\n",
    "# I_fixed = [0, k]\n",
    "I_free = [i for i in I if I.index(i) not in I_fixed]\n",
    "\n",
    "# new material for targerted with norm 1\n",
    "H_s = H[:,I_free]\n",
    "H_s_transpose = np.transpose(H_s)\n",
    "B_s = H_s @ np.linalg.inv(H_s_transpose @ H_s) @ H_s_transpose - np.eye(H_s.shape[0])\n",
    "\n",
    "m = Model('targerted least effort with norm 1')\n",
    "\n",
    "# define decision variables: a and c vectors\n",
    "\n",
    "a  = [m.addVar(name='a%d' % i) for i in range(H.shape[0])]\n",
    "c  = [m.addVar(name='c%d' % i) for i in range(H.shape[1])]\n",
    "\n",
    "a_positive = [m.addVar(lb=0.0,name='a_pos%d' % i) for i in range(H.shape[0])]\n",
    "a_negative = [m.addVar(ub=0.0,name='a_neg%d' % i) for i in range(H.shape[0])]\n",
    "\n",
    "m.update()\n",
    "\n",
    "# constraint: a[i] = a_positive[i] - a_negative[i]\n",
    "# not a constraint: |a[i]| = a_positive[i] + a_negative[i]\n",
    "for i in range(0,H.shape[0]):\n",
    "    m.addConstr(a[i] == a_positive[i] + a_negative[i])\n",
    "\n",
    "# 1st constraint: c[k] = delta\n",
    "m.addConstr(c[0] == 0)\n",
    "m.addConstr(c[k] == delta_a)\n",
    "\n",
    "# secure sensor\n",
    "# m.addConstr(a[1] == 0)\n",
    "\n",
    "# 2nd constraint: B_s*a = y (replacing a = H*c)\n",
    "c_aux = [c[i] for i in I_fixed]\n",
    "for i in range(0,H.shape[0]):\n",
    "    #m.addConstr(a[i] == np.matmul(H,c)[i])\n",
    "    m.addConstr(np.matmul(B_s,a)[i] == (B_s @ H[:,I_fixed] @ c_aux)[i])\n",
    "    \n",
    "# 3rd constraint: ... <= a[i] <= ...\n",
    "for i in range(0,H.shape[0]):\n",
    "    m.addConstr(a[i] <= a_upper_bound)\n",
    "    m.addConstr(a[i] >= a_lower_bound)\n",
    "    \n",
    "# 4th constraint: ... <= c[i] <= ...    \n",
    "for i in range(0,H.shape[1]):\n",
    "    m.addConstr(c[i] <= c_upper_bound)\n",
    "    m.addConstr(c[i] >= c_lower_bound)\n",
    "\n",
    "#m.setObjective(np.linalg.norm(a,ord=1), GRB.MINIMIZE)\n",
    "# |a[i]| = a_positive[i] - a_negative[i]\n",
    "m.setObjective(sum(z[0]-z[1] for z in zip(a_positive, a_negative)), GRB.MINIMIZE)\n",
    "\n",
    "m.update()\n",
    "\n",
    "m.optimize()\n",
    "\n",
    "#least_effort_a_norm_1 = m.getVars()[0:H.shape[0]]\n",
    "targeted_least_effort_a_norm_1 = [v.x for v in m.getVars()[:H.shape[0]]]\n",
    "# a for a single point in time\n",
    "targeted_least_effort_a_norm_1 = np.asarray(targeted_least_effort_a_norm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "66be742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning for adding constraints: zero or small (< 1e-13) coefficients, ignored\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (win64)\n",
      "Thread count: 2 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 130 rows, 54 columns and 442 nonzeros\n",
      "Model fingerprint: 0xfe6dab40\n",
      "Variable types: 34 continuous, 20 integer (20 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [9e-04, 1e+04]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e-01, 1e+02]\n",
      "Presolve removed 111 rows and 25 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 19 rows, 29 columns, 57 nonzeros\n",
      "Variable types: 13 continuous, 16 integer (16 binary)\n",
      "Found heuristic solution: objective 2.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 1: 2 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+00, best bound 2.000000000000e+00, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# Targeted least effort based on big-M, where M is a hyper-parameter\n",
    "# min sum(y) with respect to a, c, and y, where a is in z+a and c is in x+c\n",
    "# subject to:\n",
    "# a = H*c -> B_s*a = y\n",
    "# a[k] = 0.1 where this is the initial desire of the adversary i.e. add 0.1 to the k-th measurement\n",
    "# all entries of a and c should be within some limits\n",
    "# -0.1 <= a[i] <= 0.1 for all i going from 0 to #meas-1\n",
    "# -0.1 <= c[i] <= 0.1 for all i going from 0 to #nodes-1\n",
    "# y[i] where i going from 0 to #meas-1 and y[i] is integer\n",
    "# -y[i]*M <= a[i] <= y[i]*M\n",
    "# if y[i] = 0, then 0 <= a[i] <= 0\n",
    "# if y[i] = 1, then -M <= a[i] <= M\n",
    "\n",
    "# input params: H, z, M, k as integer between 0 and H.shape[0]-1, delta_a, a_lower_bound, a_upper_bound, \n",
    "# c_lower_bound, c_upper_bound\n",
    "\n",
    "from gurobipy import *\n",
    "\n",
    "delta_a = 0.1\n",
    "k = 4\n",
    "a_lower_bound, a_upper_bound, c_lower_bound, c_upper_bound = -100, 100, -100, 100\n",
    "\n",
    "M = 10**4\n",
    "\n",
    "m = Model('targeted least effort with big-M')\n",
    "\n",
    "# new material for targerted with norm 1\n",
    "I = [i for i in range(0,H.shape[1])]\n",
    "I_fixed = [0,k] # 0 always there, but \"1,2,3\" could have been \"2,4,9\"\n",
    "# I_fixed = [0, k]\n",
    "I_free = [i for i in I if I.index(i) not in I_fixed]\n",
    "\n",
    "# new material for targerted with norm 1\n",
    "H_s = H[:,I_free]\n",
    "H_s_transpose = np.transpose(H_s)\n",
    "B_s = H_s @ np.linalg.inv(H_s_transpose @ H_s) @ H_s_transpose - np.eye(H_s.shape[0])\n",
    "\n",
    "# define decision variables: a and c vectors\n",
    "\n",
    "a = [m.addVar(name='a%d' % i) for i in range(H.shape[0])]\n",
    "c = [m.addVar(name='c%d' % i) for i in range(H.shape[1])]\n",
    "y = [m.addVar(vtype=gurobipy.GRB.BINARY, name='y%d' % i) for i in range(H.shape[0])]\n",
    "\n",
    "m.update()\n",
    "\n",
    "# -y[i]*M <= a[i] <= y[i]*M\n",
    "for i in range(0,H.shape[0]):\n",
    "    m.addConstr(a[i] <= y[i]*M)\n",
    "    m.addConstr(a[i] >= -y[i]*M)\n",
    "\n",
    "# 1st constraint: c[k] = delta\n",
    "m.addConstr(c[0] == 0)\n",
    "m.addConstr(c[k] == delta_a)\n",
    "\n",
    "\n",
    "# secure sensor\n",
    "# m.addConstr(a[1] == 0)\n",
    "\n",
    "# 2nd constraint: B_s*a = y (replacing a = H*c)\n",
    "c_aux = [c[i] for i in I_fixed]\n",
    "for i in range(0,H.shape[0]):\n",
    "    #m.addConstr(a[i] == np.matmul(H,c)[i])\n",
    "    m.addConstr(np.matmul(B_s,a)[i] == (B_s @ H[:,I_fixed] @ c_aux)[i])\n",
    "    \n",
    "# 3rd constraint: ... <= a[i] <= ...\n",
    "for i in range(0,H.shape[0]):\n",
    "    m.addConstr(a[i] <= a_upper_bound)\n",
    "    m.addConstr(a[i] >= a_lower_bound)\n",
    "    \n",
    "# 4th constraint: ... <= c[i] <= ...    \n",
    "for i in range(0,H.shape[1]):\n",
    "    m.addConstr(c[i] <= c_upper_bound)\n",
    "    m.addConstr(c[i] >= c_lower_bound)\n",
    "\n",
    "#m.setObjective(np.linalg.norm(a,ord=1), GRB.MINIMIZE)\n",
    "# |a[i]| = a_positive[i] - a_negative[i]\n",
    "m.setObjective(sum(y), GRB.MINIMIZE)\n",
    "\n",
    "m.update()\n",
    "\n",
    "m.optimize()\n",
    "\n",
    "targeted_least_effort_a_big_M = [v.x for v in m.getVars()[:H.shape[0]]]\n",
    "# a for a single point in time\n",
    "targeted_least_effort_a_big_M = np.asarray(targeted_least_effort_a_big_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0feac1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 logcosh\n",
      "1 exp\n",
      "1 cube\n",
      "2 logcosh\n",
      "2 exp\n",
      "2 cube\n",
      "3 logcosh\n",
      "3 exp\n",
      "3 cube\n",
      "4 logcosh\n",
      "4 exp\n",
      "4 cube\n",
      "5 logcosh\n",
      "5 exp\n",
      "5 cube\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 logcosh\n",
      "6 exp\n",
      "6 cube\n",
      "7 logcosh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 exp\n",
      "7 cube\n",
      "8 logcosh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 exp\n",
      "8 cube\n",
      "9 logcosh\n",
      "9 exp\n",
      "9 cube\n",
      "10 logcosh\n",
      "10 exp\n",
      "10 cube\n",
      "11 logcosh\n",
      "11 exp\n",
      "11 cube\n",
      "12 logcosh\n",
      "12 exp\n",
      "12 cube\n",
      "13 logcosh\n",
      "13 exp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 cube\n",
      "14 logcosh\n",
      "14 exp\n",
      "14 cube\n",
      "15 logcosh\n",
      "15 exp\n",
      "15 cube\n",
      "16 logcosh\n",
      "16 exp\n",
      "16 cube\n",
      "17 logcosh\n",
      "17 exp\n",
      "17 cube\n",
      "18 logcosh\n",
      "18 exp\n",
      "18 cube\n",
      "19 logcosh\n",
      "19 exp\n",
      "19 cube\n",
      "20 logcosh\n",
      "20 exp\n",
      "20 cube\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "z = z_t_mat[:,0:20] # observe a time window of measurements and perturb the last time step\n",
    "\n",
    "# length of time window should be great or equal to number of measurements, so let's go for 2 times number of measurements\n",
    "\n",
    "iteration = []\n",
    "for i in range(1,z.shape[0]+1):\n",
    "    for j in ['logcosh','exp','cube']:\n",
    "        print(i,j)\n",
    "        ica = FastICA(n_components=i, random_state=0, whiten='unit-variance', fun=j)\n",
    "        y = ica.fit_transform(np.transpose(z)) # fit_transform takes as argument (n_samples, n_features)\n",
    "        G = ica.mixing_\n",
    "        deviation = np.max(np.abs(z - (G @ np.transpose(y)) - ica.mean_.reshape((-1,1))))\n",
    "        iteration.append((i,j,deviation))\n",
    "best_n_components, best_fun, best_deviation = sorted(iteration, key = lambda t: t[2])[0]\n",
    "\n",
    "if best_deviation > 1e-4:\n",
    "    print('infeasible')\n",
    "\n",
    "best_n_components = 4\n",
    "ica = FastICA(n_components=best_n_components, random_state=0, whiten='unit-variance', fun=best_fun)\n",
    "y = ica.fit_transform(np.transpose(z)) # fit_transform takes as argument (n_samples, n_components)\n",
    "G = ica.mixing_ # (n_features, n_components), it represents H*A in z = H*A*y and x = A*y\n",
    "\n",
    "delta_y = np.zeros(y.shape[1])\n",
    "sigma_2 = 0.000001\n",
    "mean = np.zeros(delta_y.shape[0])\n",
    "cov = np.eye(delta_y.shape[0])*sigma_2\n",
    "delta_y = np.random.multivariate_normal(mean, cov**2)\n",
    "a = G @ (y[-1,:] + delta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "c81cb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code segments for Brownian motion and Ornstein-Uhlenbeck process is from:\n",
    "# https://towardsdatascience.com/stochastic-processes-simulation-brownian-motion-the-basics-c1d71585d9f9\n",
    "# and\n",
    "# https://towardsdatascience.com/stochastic-processes-simulation-the-ornstein-uhlenbeck-process-e8bff820f3\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class OUParams:\n",
    "    alpha: float  # mean reversion parameter\n",
    "    gamma: float  # asymptotic mean\n",
    "    beta: float  # Brownian motion scale (standard deviation)\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_dW(T: int, random_state: Optional[int] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sample T times from a normal distribution,\n",
    "    to simulate discrete increments (dW) of a Brownian Motion.\n",
    "    Optional random_state to reproduce results.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    return np.random.normal(0.0, 1.0, T)\n",
    "\n",
    "\n",
    "def get_W(T: int, random_state: Optional[int] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate a Brownian motion discretely samplet at unit time increments.\n",
    "    Returns the cumulative sum\n",
    "    \"\"\"\n",
    "    dW = get_dW(T, random_state)\n",
    "    # cumulative sum and then make the first index 0.\n",
    "    dW_cs = dW.cumsum()\n",
    "    return np.insert(dW_cs, 0, 0)[:-1]\n",
    "\n",
    "def get_OU_process(\n",
    "    T: int,\n",
    "    OU_params: OUParams,\n",
    "    X_0: Optional[float] = None,\n",
    "    random_state: Optional[int] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    - T is the sample size.\n",
    "    - Ou_params is an instance of OUParams dataclass.\n",
    "    - X_0 the initial value for the process, if None, then X_0 is taken\n",
    "        to be gamma (the asymptotic mean).\n",
    "    Returns a 1D array.\n",
    "    \"\"\"\n",
    "    t = np.arange(T, dtype=float) # float to avoid np.exp overflow\n",
    "    exp_alpha_t = np.exp(-OU_params.alpha * t)\n",
    "    dW = get_dW(T, random_state)\n",
    "    integral_W = _get_integal_W(t, dW, OU_params)\n",
    "    _X_0 = _select_X_0(X_0, OU_params)\n",
    "    return (\n",
    "        _X_0 * exp_alpha_t\n",
    "        + OU_params.gamma * (1 - exp_alpha_t)\n",
    "        + OU_params.beta * exp_alpha_t * integral_W\n",
    "    )\n",
    "\n",
    "\n",
    "def _select_X_0(X_0_in: Optional[float], OU_params: OUParams) -> float:\n",
    "    \"\"\"Returns X_0 input if not none, else gamma (the long term mean).\"\"\"\n",
    "    if X_0_in is not None:\n",
    "        return X_0_in\n",
    "    return OU_params.gamma\n",
    "\n",
    "\n",
    "def _get_integal_W(\n",
    "    t: np.ndarray, dW: np.ndarray, OU_params: OUParams\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Integral with respect to Brownian Motion (W), ∫...dW.\"\"\"\n",
    "    exp_alpha_s = np.exp(OU_params.alpha * t)\n",
    "    integral_W = np.cumsum(exp_alpha_s * dW)\n",
    "    return np.insert(integral_W, 0, 0)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "9bc46d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OU_proc = []\n",
    "for i in range(0,len(net.load)):\n",
    "    alpha_rv = np.abs(np.random.normal(0.01, 0.01))\n",
    "    beta_rv = np.abs(np.random.normal(0.005, 0.01))\n",
    "    OU_params = OUParams(alpha=alpha_rv, gamma=0.0, beta=beta_rv)\n",
    "    OU_proc.append(get_OU_process(100, OU_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f1dea0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "p_mw_init = net.load.p_mw.values.copy()\n",
    "OU_proc = np.asarray(OU_proc)\n",
    "x_t_mat = []\n",
    "for t in range(0,100):\n",
    "    delta_load = p_mw_init * (1+OU_proc[:,t])\n",
    "    net.load.p_mw = Series(delta_load)\n",
    "    pp.rundcpp(net)\n",
    "    x_t_mat.append(net.res_bus.va_degree.to_numpy()/180*np.pi)\n",
    "x_t_mat = np.transpose(np.asarray(x_t_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "ed19548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5000\n",
    "Z = z_t_mat[:,0:T]\n",
    "z_mean = (1/(Z.shape[1]-1))*np.sum(Z,axis=1)\n",
    "list_outer = []\n",
    "for i in range(0,T):\n",
    "    list_outer.append(np.outer(Z[:,i] - z_mean,Z[:,i] - z_mean))\n",
    "sigma_z = sum(list_outer)/(T-1)\n",
    "lamdas, eigenvectors = np.linalg.eig(sigma_z)\n",
    "\n",
    "p = Z.shape[0]/Z.shape[1] # the ratio of measurements over number of snapshots\n",
    "\n",
    "#s = []\n",
    "#threshold = (1 + np.sqrt(p))**2\n",
    "#for i in range(0,lamdas.shape[0]):\n",
    "#    if lamdas[i] > threshold:\n",
    "#        s.append(i)\n",
    "\n",
    "mu = []\n",
    "for i in range(0,lamdas.shape[0]):\n",
    "    mu.append((lamdas[i]+1-p+np.sqrt((lamdas[i]+1-p)**2-4*lamdas[i]))/2 - 1)\n",
    "\n",
    "omegas = []\n",
    "for i in range(0,len(mu)):\n",
    "    omegas.append((1-p/mu[i]**2)/(1+p/mu[i]))\n",
    "    \n",
    "# scenario 1a\n",
    "#np.random.choice(len(s),1)\n",
    "i = np.random.choice(lamdas.shape[0],1)[0]\n",
    "tau = 0.3 # an input parameter\n",
    "c_i = np.sqrt(tau/((1e-3)**2*omegas[i]/mu[i])) # 1e-3 is the noise_factor\n",
    "a = c_i * eigenvectors[:,i]\n",
    "\n",
    "# scenario 1b (use the largest eigenvalue)\n",
    "tau = 0.3 # an input parameter\n",
    "i = np.argmax(lamdas)\n",
    "c_i = np.sqrt(tau/((1e-3)**2*omegas[i]/mu[i])) # 1e-3 is the noise_factor\n",
    "a = c_i * eigenvectors[:,i]\n",
    "\n",
    "# scenario 2\n",
    "tau = 0.3 # an input parameter\n",
    "c_i = []\n",
    "for i in range(0,lamdas.shape[0]):\n",
    "    c_i.append(np.sqrt(tau/((1e-3)**2*lamdas.shape[0]*omegas[i]/mu[i]))) # 1e-3 is the noise_factor\n",
    "c_i = np.asarray(c_i)\n",
    "a = eigenvectors @ c_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "39568c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm-2 squared of delta_thetas could probabilistically become above tau\n",
    "# np.linalg.norm(delta_theta, ord=2)**2\n",
    "# where delta_theta is x_normal - x_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
